{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zehra-Khuwaja/DS-A-20sw065-lab-tasks/blob/main/LAB_12_text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCB890DQiqLd"
      },
      "source": [
        "DS&A LAB-12\n",
        "20SW041\n",
        "SECTION-I\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXg_OPP7G59B"
      },
      "source": [
        "# Text Processing\n",
        "\n",
        "## Capturing Text Data\n",
        "\n",
        "### Plain Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Gy7zDRG59K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Read in a plain text file\n",
        "with open(os.path.join(\"data\", \"hieroglyph.txt\"), \"r\") as f:\n",
        "    text = f.read()\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-o9Wc25G59O"
      },
      "source": [
        "### Tabular Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHqSRCEPG59Q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract text column from a dataframe\n",
        "df = pd.read_csv(os.path.join(\"data\", \"news.csv\"))\n",
        "df.head()[['publisher', 'title']]\n",
        "\n",
        "# Convert text column to lowercase\n",
        "df['title'] = df['title'].str.lower()\n",
        "df.head()[['publisher', 'title']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTkqnlFG59R"
      },
      "source": [
        "### Online Resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAbiK2XZG59T",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Fetch data from a REST API\n",
        "r = requests.get(\n",
        "    \"https://quotes.rest/qod.json\")\n",
        "res = r.json()\n",
        "print(json.dumps(res, indent=4))\n",
        "\n",
        "# Extract relevant object and field\n",
        "q = res[\"contents\"][\"quotes\"][0]\n",
        "print(q[\"quote\"], \"\\n--\", q[\"author\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUuR9aR7G59U"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJddrgujG59V",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Fetch a web page\n",
        "r = requests.get(\"https://news.ycombinator.com\")\n",
        "print(r.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24gp_p8pG59X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Remove HTML tags using RegEx\n",
        "pattern = re.compile(r'<.*?>')  # tags look like <...>\n",
        "print(pattern.sub('', r.text))  # replace them with blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taniX2HSG59Y"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Remove HTML tags using Beautiful Soup library\n",
        "soup = BeautifulSoup(r.text, \"html5lib\")\n",
        "print(soup.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZy5AFjrG59a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Find all articles\n",
        "summaries = soup.find_all(\"tr\", class_=\"athing\")\n",
        "summaries[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW4_MdQ0G59c"
      },
      "outputs": [],
      "source": [
        "# Extract title\n",
        "summaries[0].find(\"a\", class_=\"storylink\").get_text().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfBG9OMLG59d"
      },
      "outputs": [],
      "source": [
        "# Find all articles, extract titles\n",
        "articles = []\n",
        "summaries = soup.find_all(\"tr\", class_=\"athing\")\n",
        "for summary in summaries:\n",
        "    title = summary.find(\"a\", class_=\"storylink\").get_text().strip()\n",
        "    articles.append((title))\n",
        "\n",
        "print(len(articles), \"Article summaries found. Sample:\")\n",
        "print(articles[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zhi2M9J8G59f"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZUYirLMG59g"
      },
      "source": [
        "### Case Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3bcoFMGG59g",
        "outputId": "1b8cba26-3f1e-4328-8db9-943dfd2f3ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In today's business world, smart data-driven decisions are the number one priority. For this reason, companies track, monitor, and record information 24/7. The good news is there is plenty of public data on servers that can help businesses stay competitive. The process of extracting data from web pages manually can be tiring, time-consuming, error-prone, and sometimes even impossible. That is why most web data analysis efforts use automated tools. Web scraping is an automated method of collecting data from web pages. Data is extracted from web pages using software called web scrapers, which are basically web bots.\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"In today's business world, smart data-driven decisions are the number one priority. For this reason, companies track, monitor, and record information 24/7. The good news is there is plenty of public data on servers that can help businesses stay competitive.\\xa0The process of extracting data from web pages manually can be tiring, time-consuming, error-prone, and sometimes even impossible. That is why most web data analysis efforts use automated tools.\\xa0Web scraping is an automated method of collecting data from web pages. Data is extracted from web pages using software called web scrapers, which are basically web bots.\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXoHq54jG59h",
        "outputId": "3bf8dbed-6b70-4734-c5e4-a316d980dfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in today's business world, smart data-driven decisions are the number one priority. for this reason, companies track, monitor, and record information 24/7. the good news is there is plenty of public data on servers that can help businesses stay competitive. the process of extracting data from web pages manually can be tiring, time-consuming, error-prone, and sometimes even impossible. that is why most web data analysis efforts use automated tools. web scraping is an automated method of collecting data from web pages. data is extracted from web pages using software called web scrapers, which are basically web bots.\n"
          ]
        }
      ],
      "source": [
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaxxaSVhG59i"
      },
      "source": [
        "### Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEY1a-sDG59j",
        "outputId": "40453a98-bf90-477d-bb53-53f88980a391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in today s business world  smart data driven decisions are the number one priority  for this reason  companies track  monitor  and record information 24 7  the good news is there is plenty of public data on servers that can help businesses stay competitive  the process of extracting data from web pages manually can be tiring  time consuming  error prone  and sometimes even impossible  that is why most web data analysis efforts use automated tools  web scraping is an automated method of collecting data from web pages  data is extracted from web pages using software called web scrapers  which are basically web bots \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Remove punctuation characters\n",
        "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcTKfuNtG59j"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBesyQF6G59k",
        "outputId": "480fba85-97e9-4429-c7d4-f73c86e7df68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['in', 'today', 's', 'business', 'world', 'smart', 'data', 'driven', 'decisions', 'are', 'the', 'number', 'one', 'priority', 'for', 'this', 'reason', 'companies', 'track', 'monitor', 'and', 'record', 'information', '24', '7', 'the', 'good', 'news', 'is', 'there', 'is', 'plenty', 'of', 'public', 'data', 'on', 'servers', 'that', 'can', 'help', 'businesses', 'stay', 'competitive', 'the', 'process', 'of', 'extracting', 'data', 'from', 'web', 'pages', 'manually', 'can', 'be', 'tiring', 'time', 'consuming', 'error', 'prone', 'and', 'sometimes', 'even', 'impossible', 'that', 'is', 'why', 'most', 'web', 'data', 'analysis', 'efforts', 'use', 'automated', 'tools', 'web', 'scraping', 'is', 'an', 'automated', 'method', 'of', 'collecting', 'data', 'from', 'web', 'pages', 'data', 'is', 'extracted', 'from', 'web', 'pages', 'using', 'software', 'called', 'web', 'scrapers', 'which', 'are', 'basically', 'web', 'bots']\n"
          ]
        }
      ],
      "source": [
        "# Split text into tokens (words)\n",
        "words = text.split()\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKFyHoKZG59l"
      },
      "source": [
        "### NLTK: Natural Language ToolKit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcFJR9GcG59l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.data.path.append(os.path.join(os.getcwd(), \"nltk_data\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHrxoX6DG59m",
        "outputId": "dae8f906-ed05-46a4-e5b9-9b3f0d80eb91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data is extracted from web pages using software called web scrapers, which are basically web bots.\n"
          ]
        }
      ],
      "source": [
        "# Another sample text\n",
        "text = \"Data is extracted from web pages using software called web scrapers, which are basically web bots.\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8zxFt9ld9_J",
        "outputId": "e10aaf48-2218-4bfa-b602-6c75d5578d74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chFzSBaIG59m",
        "outputId": "bb3f36f7-10b8-425e-eb96-498d0f8b3787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Data', 'is', 'extracted', 'from', 'web', 'pages', 'using', 'software', 'called', 'web', 'scrapers', ',', 'which', 'are', 'basically', 'web', 'bots', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split text into words using NLTK\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-Vs7t1G59n",
        "outputId": "315fe25b-e738-4d26-f74e-e621a4313307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Data is extracted from web pages using software called web scrapers, which are basically web bots.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Split text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBe4GtSmeKKv",
        "outputId": "283717d5-2a01-40f5-a15a-436c19e687c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO9XYFtVG59o",
        "outputId": "25079123-6f69-457d-b709-f067b1bfa456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# List stop words\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdynKMMTG59p",
        "outputId": "cfb5ca9a-a4fe-44b9-cfc1-8441bd6730e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['in', 'today', 's', 'business', 'world', 'smart', 'data', 'driven', 'decisions', 'are', 'the', 'number', 'one', 'priority', 'for', 'this', 'reason', 'companies', 'track', 'monitor', 'and', 'record', 'information', '24', '7', 'the', 'good', 'news', 'is', 'there', 'is', 'plenty', 'of', 'public', 'data', 'on', 'servers', 'that', 'can', 'help', 'businesses', 'stay', 'competitive']\n"
          ]
        }
      ],
      "source": [
        "# Reset text\n",
        "text = \"In today's business world, smart data-driven decisions are the number one priority. For this reason, companies track, monitor, and record information 24/7. The good news is there is plenty of public data on servers that can help businesses stay competitive. \"\n",
        "\n",
        "# Normalize it\n",
        "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
        "\n",
        "# Tokenize it\n",
        "words = text.split()\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGI6p4o-G59p",
        "outputId": "983a8fc5-d1b4-45bc-c0de-335de90ea909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['today', 'business', 'world', 'smart', 'data', 'driven', 'decisions', 'number', 'one', 'priority', 'reason', 'companies', 'track', 'monitor', 'record', 'information', '24', '7', 'good', 'news', 'plenty', 'public', 'data', 'servers', 'help', 'businesses', 'stay', 'competitive']\n"
          ]
        }
      ],
      "source": [
        "# Remove stop words\n",
        "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAdBH09jG59q"
      },
      "source": [
        "### Sentence Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8mN5Q-KG59q",
        "outputId": "1ffb6398-a4a3-4317-a496-9aa58e0e2df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V shot) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V shot)\n",
            "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Define a custom grammar\n",
        "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'an' | 'my'\n",
        "N -> 'elephant' | 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\")\n",
        "parser = nltk.ChartParser(my_grammar)\n",
        "\n",
        "# Parse a sentence\n",
        "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP6BE3ViG59r"
      },
      "source": [
        "## Stemming & Lemmatization\n",
        "\n",
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guV3wpPWG59s",
        "outputId": "34b70562-34c9-4f73-dd80-70e74d773471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['today', 'busi', 'world', 'smart', 'data', 'driven', 'decis', 'number', 'one', 'prioriti', 'reason', 'compani', 'track', 'monitor', 'record', 'inform', '24', '7', 'good', 'news', 'plenti', 'public', 'data', 'server', 'help', 'busi', 'stay', 'competit']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Reduce words to their stems\n",
        "stemmed = [PorterStemmer().stem(w) for w in words]\n",
        "print(stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0KB_1sqG59t"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flYw7WJyefqq",
        "outputId": "46b3062e-7f61-41a3-d5d8-ed9bb662ae27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nfVzMV2G59t",
        "outputId": "37ce176e-dd7b-4a3a-fca6-b32d13ecaec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['today', 'business', 'world', 'smart', 'data', 'driven', 'decision', 'number', 'one', 'priority', 'reason', 'company', 'track', 'monitor', 'record', 'information', '24', '7', 'good', 'news', 'plenty', 'public', 'data', 'server', 'help', 'business', 'stay', 'competitive']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "# Reduce words to their root form\n",
        "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
        "print(lemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls4LCWrsG59u",
        "outputId": "12b0f0fd-59a2-45d1-d53f-b107e3c8d59d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['today', 'business', 'world', 'smart', 'data', 'drive', 'decision', 'number', 'one', 'priority', 'reason', 'company', 'track', 'monitor', 'record', 'information', '24', '7', 'good', 'news', 'plenty', 'public', 'data', 'server', 'help', 'business', 'stay', 'competitive']\n"
          ]
        }
      ],
      "source": [
        "# Lemmatize verbs by specifying pos\n",
        "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
        "print(lemmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rni6mW8mG59v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:nlpnd]",
      "language": "python",
      "name": "conda-env-nlpnd-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
